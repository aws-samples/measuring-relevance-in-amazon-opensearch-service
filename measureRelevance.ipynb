{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243a4d287bbafe5a",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63fd1464af8131a",
   "metadata": {},
   "source": [
    "Install the libraries that are not installed by default in the conda_python3 kernel used in the Sagemaker Notebook used for this tuturial. If you are running this tutorial locally, you may beed to install aditional libraries like boto3 and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b44b9ffccec7a",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4444bb8b0259db",
   "metadata": {},
   "source": [
    "We use the Cranfield dataset, a small corpus of 1400 scientific abstracts. It is a slight modification of the dataset that you can download from https://ir-datasets.com/cranfield.html. We have modified the queries \"query_id\" field to get values 1-225 to match qrels \"query_id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c127fae69ea2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:41:19.628429Z",
     "start_time": "2024-09-27T10:41:12.192807Z"
    }
   },
   "outputs": [],
   "source": [
    "from measureRelevance import download_s3_file\n",
    "\n",
    "bucket_name = 'aws-blogs-artifacts-public'\n",
    "s3_folder = 'BDB-4664/'  # Folder (prefix) in S3, ensure it ends with a '/'\n",
    "\n",
    "# Documents\n",
    "s3_docs_key = 'BDB-4664/docs.zip'\n",
    "# Queries\n",
    "s3_queries_key = 'BDB-4664/queries.zip'\n",
    "# Queries relevancies\n",
    "s3_qrels_key = 'BDB-4664/qrels.zip'\n",
    "\n",
    "# Download files\n",
    "download_s3_file(bucket_name, s3_docs_key)\n",
    "download_s3_file(bucket_name, s3_queries_key)\n",
    "download_s3_file(bucket_name, s3_qrels_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037a3d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:42:52.217616Z",
     "start_time": "2024-09-27T10:42:51.728260Z"
    }
   },
   "outputs": [],
   "source": [
    "!unzip -o docs\n",
    "!unzip -o queries\n",
    "!unzip -o qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6d667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:43:01.385085Z",
     "start_time": "2024-09-27T10:43:01.359578Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "docs = load_from_disk(\"docs\")\n",
    "queries = load_from_disk(\"queries\")\n",
    "qrels = load_from_disk(\"qrels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229caf8dccac712",
   "metadata": {},
   "source": [
    "## Load OS Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61664f29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:43:12.433563Z",
     "start_time": "2024-09-27T10:43:11.722727Z"
    }
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "# Create OS client\n",
    "domain_name = 'measure-relevance-os-domain'\n",
    "region = 'us-east-1'\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "\n",
    "boto3_client_os = boto3.client('opensearch')\n",
    "domain_host = boto3_client_os.describe_domain(DomainName='{}'.format(domain_name))['DomainStatus']['Endpoint']\n",
    "\n",
    "\n",
    "os_client = OpenSearch(\n",
    "            hosts=[{'host': domain_host, 'port': 443}],\n",
    "            http_auth=auth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668edf1893af4e8b",
   "metadata": {},
   "source": [
    "## Ingest documents to OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72947bbdb58360",
   "metadata": {},
   "source": [
    "We ingest the Cranfield documents using the parallel version of the bulk helper of the OpenSearch Python client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4815e59c0e208d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:43:22.019584Z",
     "start_time": "2024-09-27T10:43:19.663059Z"
    }
   },
   "outputs": [],
   "source": [
    "from opensearchpy.helpers import parallel_bulk\n",
    "\n",
    "index_name = 'cranfield'\n",
    "\n",
    "\n",
    "def _generate_data():\n",
    "    for doc in docs:\n",
    "        yield {\"_index\": index_name, \"_id\": doc['doc_id'], \"title\": doc['title'],\n",
    "               \"author\": doc['author'], \"bib\": doc['bib'], \"text\": doc['text']}\n",
    "\n",
    "\n",
    "succeeded = []\n",
    "failed = []\n",
    "for success, item in parallel_bulk(os_client, actions=_generate_data()):\n",
    "    if success:\n",
    "        succeeded.append(item)\n",
    "    else:\n",
    "        failed.append(item)\n",
    "\n",
    "if len(failed) > 0:\n",
    "    print(f\"There were {len(failed)} errors:\")\n",
    "    for item in failed:\n",
    "        print(item[\"index\"][\"error\"])\n",
    "\n",
    "if len(succeeded) > 0:\n",
    "    print(f\"Bulk-inserted {len(succeeded)} items (streaming_bulk).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b3a12eccb6b7f",
   "metadata": {},
   "source": [
    "## Query the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60616d26dd8b7178",
   "metadata": {},
   "source": [
    "We do the 225 queries them using the Search API operation lets you execute a search request on your indexed documents. For our examples, we will do a full-text search using the match query with the default parameters. We limit our response results to the 100 most relevant ranked documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f6483997657e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:50:24.577180Z",
     "start_time": "2024-09-27T10:50:24.493507Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "relevant_docs = defaultdict(list)\n",
    "\n",
    "for qrel in qrels:\n",
    "    relevant_docs[qrel['query_id']].append(qrel['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8d8cb1e165b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T10:50:51.479394Z",
     "start_time": "2024-09-27T10:50:45.520946Z"
    }
   },
   "outputs": [],
   "source": [
    "from measureRelevance import list_of_docs\n",
    "\n",
    "#to get list of all documents\n",
    "no_of_top = 100\n",
    "list_of_documents = list_of_docs(no_of_top, queries, os_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6f58c56b1cdb4",
   "metadata": {},
   "source": [
    "## Calculate recall, precision and F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ba18482150a43",
   "metadata": {},
   "source": [
    "Recall: Fraction of relevant documents that are retrieved\n",
    "\n",
    "- Recall = #(relevant items retrieved)/#(relevant items) = P(retrieved|relevant)\n",
    "\n",
    "Precision (P) is the fraction of retrieved documents that are relevant\n",
    "\n",
    "- Precision = #(relevant items retrieved)/#(retrieved items) = P(relevant|retrieved)\n",
    "\n",
    "F1: Harmonic mean of precision and recall\n",
    "\n",
    "- F1 = 2 * (Precision * Recall)/(Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332370fb7092396",
   "metadata": {},
   "source": [
    "We start by calculating recall, precision and F1 with K=10. These values are valuable, but the ultimate goal is to find the K that mazimises these metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eeaf30f5dc77c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:29:16.748623Z",
     "start_time": "2024-09-09T13:29:16.695091Z"
    }
   },
   "outputs": [],
   "source": [
    "from measureRelevance import calculate_precision, calculate_recall, calculate_f1\n",
    "import statistics\n",
    "\n",
    "top = [10, 50, 100, 500]\n",
    "no_of_top = top[1]\n",
    "\n",
    "precision = calculate_precision(no_of_top, list_of_documents, relevant_docs)\n",
    "recall = calculate_recall(no_of_top, list_of_documents, relevant_docs)\n",
    "f1 = calculate_f1(precision, recall)\n",
    "\n",
    "print(\"Mean Precision:{}\".format(statistics.mean(precision)))\n",
    "print(\"Mean Recall:{}\".format(statistics.mean(recall)))\n",
    "print(\"Mean F1:{}\".format(statistics.mean(f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6708fd2e1b4774",
   "metadata": {},
   "source": [
    "## Plot recall, precision and F1 @K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ca5050326f208",
   "metadata": {},
   "source": [
    "We now calculate recall, precision an F1 accross different K values. In this case, from K=1 to K=30 to find the values of K that mazimises each of the metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca9e35be96ff83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T15:34:00.832433Z",
     "start_time": "2024-09-09T15:33:59.142686Z"
    }
   },
   "outputs": [],
   "source": [
    "from measureRelevance import plot_recall_vs_k, plot_precision_vs_k, plot_f1_vs_k, plot_precision_recall_f1_vs_k\n",
    "\n",
    "# Set the maximum value of K\n",
    "max_k = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a22e193dba17",
   "metadata": {},
   "source": [
    "Recall answers the question: \"Of all the relevant documents in the dataset, how many did the system find?\" To maximize recall, you would want your system to retrieve as many relevant documents as possible. This happens as you increase the number of documents retrieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80028fc228aab127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot recall vs K\n",
    "plot_recall_vs_k(max_k, list_of_documents, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6431bea316c049",
   "metadata": {},
   "source": [
    "Precision answers the question: \"Of the documents retrieved, how many were actually relevant?\" To maximize precision, you would want your system to retrieve only the most relevant documents and avoid retrieving irrelevant ones. As you increase the number of documents retrieved, the precision lowers because between your retrieved documents appear some that are not relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384c44bda7bbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot precision vs K\n",
    "plot_precision_vs_k(max_k, list_of_documents, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf98854f7f2346",
   "metadata": {},
   "source": [
    "The F1 score is designed to combine precision and recall into a single metric. It is particularly useful when both metrics need to be balanced and neither can be prioritized over the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88651cb0e81a428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot F1 score vs K\n",
    "plot_f1_vs_k(max_k, list_of_documents, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235e58d402e2b38",
   "metadata": {},
   "source": [
    "Let's plot recall, precision and F1. We can see how recall increases as K increases while precision decreases. The F1 score takes both recall and precision values into consideration to find a K value that maximises both metrics. \n",
    "- K=6 achieves the highest F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593a7e6fe4655e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot Precision, Recall, and F1 vs K\n",
    "plot_precision_recall_f1_vs_k(max_k, list_of_documents, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2299e8bff7da7e",
   "metadata": {},
   "source": [
    "## Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276acfb5c8dc733",
   "metadata": {},
   "source": [
    "Averages the AP scores across multiple queries, considering only the top K retrieved documents.\n",
    "- K=2 achieves the highest MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76ace46320f62c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:29:19.143152Z",
     "start_time": "2024-09-09T13:29:18.826686Z"
    }
   },
   "outputs": [],
   "source": [
    "from measureRelevance import plot_map_vs_k\n",
    "\n",
    "# Set the maximum value of K\n",
    "max_k = 30\n",
    "\n",
    "# Call the function to plot MAP@K vs K\n",
    "plot_map_vs_k(max_k, list_of_documents, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a0721bfaf002",
   "metadata": {},
   "source": [
    "## Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb93b9b540c8ca",
   "metadata": {},
   "source": [
    "The relK variable is different this time. It is a range of relevance ranks where *0* is the least relevant, and some higher value is the most relevant. relK = qrel['relevance']\n",
    "\n",
    "* In our case -1 is non-relevant, so we don't use it as relK\n",
    "* We then have 1,2,3,4 refactored to 0,1,2,3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3c4cd4eda35da",
   "metadata": {},
   "source": [
    "We start by calculating the Discounted Cumulative Gain (DCG) and the Ideal DCG (IDCG). The IDCG represents the best posible DCG, in which the K documents retrieved are ordered in decreasing relevancy. IDCG is used to normalize the DCG results. \n",
    "\n",
    "When plotting the results, we achieved the highest NDCG with K=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e49abc2dd16890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:29:19.825868Z",
     "start_time": "2024-09-09T13:29:19.161669Z"
    }
   },
   "outputs": [],
   "source": [
    "from measureRelevance import plot_ndcg_dcg_idcg\n",
    "\n",
    "query_id = 1\n",
    "max_k = 30\n",
    "\n",
    "plot_ndcg_dcg_idcg(list_of_documents, query_id, qrels, max_k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
